#println();
#+1 because we have to process last one as well
if(colCount+1<= lead)
{
quit = TRUE
break
}
i=row
#finding the row with element at lead position is non zero to transfer it up.
while(! quit && (m[i,lead] < thres) && (m[i,lead] > -1*thres) )
{
# this is just for resetting the row when starting to search in next column for leading non-zero
#element
#this will increase the row number if zero
#below only if it is the last row than increase the column not row
if(rowCount < i+1)
{
#resetting the row to start over this time for new lead
i=row;
lead = lead+1;
if(colCount+1 <= lead)
{
quit = TRUE;
break;
}
}
else {
i = i+1;
}
}
if(!quit)
{
#swaping
temp = m[i,]
m[i,] = m[row,]
m[row,] = temp
#swapRows(m, i, row);
if(!((m[row,lead] < thres) && (m[row,lead] > -1*thres)) )
{
m[row,] = m[row,]/m[row,lead]
}  #  multiplyRow(m, row, 1.0f / m[row][lead]);
i = 1
while( i <= rowCount)
{
if(i != row)
{  m[i,] = m[i,] - m[i,lead]*m[row,]
}#subtractRows(m, m[i][lead], row, i);
i = i+1
}
}
row = row+1;
#lead = lead+1;#moving the index so that do not stuck on same
}
#yahoo
count =0;
final = m[rowSums(m^2)>0,]
lol = dim(final);
lol[1] = lol[1]+1;
null = final[,lol[1]:lol[2]]; #to ignore the null rows
null = t(null)
null = -1*null;
crntdim = dim(null);
temp = lol[2]-lol[1]+1;#strength of the null space of a
tempmat = diag(temp);
null2 = cbind(null,tempmat)
null2 = t(null2)
final2 = t(final)
debugSource('H:/R workspace/first.R')
View(m)
View(mat)
View(tempmat)
View(tempmat)
#!/usr/bin/env Rscript
# your code must have this first line.
# Train code for logistic regression part goes here
args <- commandArgs(trailingOnly = TRUE)
file_name <- args[1]
modeltheta <- args[2]
file_name  = "H:/R workspace/assnmt2/files1/input2.csv";
modeltheta = "sol.txt";
input <- read.table(file_name,header = FALSE,sep = ",")
mat <- as.matrix(input);
dimn = dim(mat)
m = dimn[1]
n = dimn[2]-1
y = mat[,n+1]
mat = mat[,-(n+1)]
x = t(t(mat));
##################create more feature here if want
#for(j in 1:n){
#}
#dimn = dim(x)
#m = dimn[1]
#n = dimn[2]
#-----------------------------------------------------#
max = 0;
for(i in y){
if(i>max){
max =i;
}
}
til = matrix(1:max*m,m,max)
til[,] = 0;
h = NROW(y)
for(p in 1:h){
yi = y[p]
til[p,yi] = 1;
}
#initializing theta
classes = max;
feature = n
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .01;
View(theta)
View(theta2)
View(til)
for(a in 1:classes){
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
View(theta)
View(til)
View(x)
#!/usr/bin/env Rscript
# your code must have this first line.
# Train code for logistic regression part goes here
args <- commandArgs(trailingOnly = TRUE)
file_name <- args[1]
modeltheta <- args[2]
file_name  = "H:/R workspace/assnmt2/files1/input2.csv";
modeltheta = "sol.txt";
input <- read.table(file_name,header = FALSE,sep = ",")
mat <- as.matrix(input);
dimn = dim(mat)
m = dimn[1]
n = dimn[2]-1
y = mat[,n+1]
mat = mat[,-(n+1)]
x = t(t(mat));
##################create more feature here if want
#for(j in 1:n){
#}
#dimn = dim(x)
#m = dimn[1]
#n = dimn[2]
#-----------------------------------------------------#
max = 0;
for(i in y){
if(i>max){
max =i;
}
}
til = matrix(1:max*m,m,max)
til[,] = 0;
h = NROW(y)
for(p in 1:h){
yi = y[p]
til[p,yi] = 1;
}
#initializing theta
classes = max;
feature = n
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .01;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
a =1;
b =1;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
View(theta2)
b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
example = 1;
alpha = .0001;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#a =1;
#b =1;
for(a in 1:classes){
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
View(theta)
for(a in 1:classes){
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
View(theta)
for(a in 1:classes){
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
View(theta2)
classes = max;
feature = n
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .0001;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#a =1;
#b =1;
for(a in 1:classes){
denom = 0
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
classes = max;
feature = n
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .0001;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#a =1;
#b =1;
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
for(a in 1:classes){
denom = 0
num= 0;
for(c in 1:classes){
denom = denom + 2.72^(t(theta[c,])%*%x[example,])
}
for(b in 1:feature){
#b=b+1;
num = 2.72^(t(theta[a,])%*%x[example,])
prob = num/denom
gradient = x[example,b]*(til[example,a]-prob)
theta2[a,b] = theta[a,b] - alpha*gradient;
}
}
theta = theta2;
example = example+1
#!/usr/bin/env Rscript
# your code must have this first line.
# Train code for logistic regression part goes here
args <- commandArgs(trailingOnly = TRUE)
file_name <- args[1]
modeltheta <- args[2]
file_name  = "H:/R workspace/assnmt2/files1/input22.csv";
modeltheta = "sol.txt";
input <- read.table(file_name,header = FALSE,sep = ",")
mat <- as.matrix(input);
dimn = dim(mat)
m = dimn[1]
n = dimn[2]-1
y = mat[,n+1]
mat = mat[,-(n+1)]
x = t(t(mat));
##################create more feature here if want
#for(j in 1:n){
#}
#dimn = dim(x)
#m = dimn[1]
#n = dimn[2]
#-----------------------------------------------------#
max = 0;
for(i in y){
if(i>max){
max =i;
}
}
til = matrix(1:max*m,m,max)
til[,] = 0;
h = NROW(y)
for(p in 1:h){
yi = y[p]
til[p,yi] = 1;
}
#initializing theta
classes = max;
feature = n
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .0001;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#a =1;
#b =1;
#!/usr/bin/env Rscript
# your code must have this first line.
# Train code for logistic regression part goes here
args <- commandArgs(trailingOnly = TRUE)
file_name <- args[1]
modeltheta <- args[2]
file_name  = "H:/R workspace/assnmt2/files1/input22.csv";
modeltheta = "sol.txt";
input <- read.table(file_name,header = FALSE,sep = ",")
mat <- as.matrix(input);
dimn = dim(mat)
m = dimn[1]
n = dimn[2]-1
y = mat[,n+1]
mat = mat[,-(n+1)]
x = t(t(mat));
##################create more feature here if want
#for(j in 1:n){
#}
#dimn = dim(x)
#m = dimn[1]
#n = dimn[2]
#-----------------------------------------------------#
max = 0;
for(i in y){
if(i>max){
max =i;
}
}
til = matrix(1:max*m,m,max)
til[,] = 0;
h = NROW(y)
for(p in 1:h){
yi = y[p]
til[p,yi] = 1;
}
#initializing theta
classes = max;
feature = n
#count occurnce
count = sum(til[,1])
example = 1;
alpha = .0001;
theta = matrix(1:max*n,classes,feature)
##########################7 rows x   54 column
theta[,] = 0;
theta2 = theta;
#a =1;
#b =1;
write_csv(predictions, "rf_benchmark.csv")
setwd(H:/R workspace/assnmt4)
setwd("H:/R workspace/assnmt4")
labels <- as.factor(train[rows,1])
cle
clr
clear
